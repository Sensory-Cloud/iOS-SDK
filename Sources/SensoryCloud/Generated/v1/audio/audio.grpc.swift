//
// DO NOT EDIT.
//
// Generated by the protocol buffer compiler.
// Source: v1/audio/audio.proto
//

//
// Copyright 2018, gRPC Authors All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
import GRPC
import NIO
import NIOConcurrencyHelpers
import SwiftProtobuf


/// Handles the retrieval and management of audio models
///
/// Usage: instantiate `Sensory_Api_V1_Audio_AudioModelsClient`, then call methods of this protocol to make API calls.
public protocol Sensory_Api_V1_Audio_AudioModelsClientProtocol: GRPCClient {
  var serviceName: String { get }
  var interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol? { get }

  func getModels(
    _ request: Sensory_Api_V1_Audio_GetModelsRequest,
    callOptions: CallOptions?
  ) -> UnaryCall<Sensory_Api_V1_Audio_GetModelsRequest, Sensory_Api_V1_Audio_GetModelsResponse>
}

extension Sensory_Api_V1_Audio_AudioModelsClientProtocol {
  public var serviceName: String {
    return "sensory.api.v1.audio.AudioModels"
  }

  /// Get available models for enrollment and authentication
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  ///
  /// - Parameters:
  ///   - request: Request to send to GetModels.
  ///   - callOptions: Call options.
  /// - Returns: A `UnaryCall` with futures for the metadata, status and response.
  public func getModels(
    _ request: Sensory_Api_V1_Audio_GetModelsRequest,
    callOptions: CallOptions? = nil
  ) -> UnaryCall<Sensory_Api_V1_Audio_GetModelsRequest, Sensory_Api_V1_Audio_GetModelsResponse> {
    return self.makeUnaryCall(
      path: Sensory_Api_V1_Audio_AudioModelsClientMetadata.Methods.getModels.path,
      request: request,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeGetModelsInterceptors() ?? []
    )
  }
}

#if compiler(>=5.6)
@available(*, deprecated)
extension Sensory_Api_V1_Audio_AudioModelsClient: @unchecked Sendable {}
#endif // compiler(>=5.6)

@available(*, deprecated, renamed: "Sensory_Api_V1_Audio_AudioModelsNIOClient")
public final class Sensory_Api_V1_Audio_AudioModelsClient: Sensory_Api_V1_Audio_AudioModelsClientProtocol {
  private let lock = Lock()
  private var _defaultCallOptions: CallOptions
  private var _interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol?
  public let channel: GRPCChannel
  public var defaultCallOptions: CallOptions {
    get { self.lock.withLock { return self._defaultCallOptions } }
    set { self.lock.withLockVoid { self._defaultCallOptions = newValue } }
  }
  public var interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol? {
    get { self.lock.withLock { return self._interceptors } }
    set { self.lock.withLockVoid { self._interceptors = newValue } }
  }

  /// Creates a client for the sensory.api.v1.audio.AudioModels service.
  ///
  /// - Parameters:
  ///   - channel: `GRPCChannel` to the service host.
  ///   - defaultCallOptions: Options to use for each service call if the user doesn't provide them.
  ///   - interceptors: A factory providing interceptors for each RPC.
  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self._defaultCallOptions = defaultCallOptions
    self._interceptors = interceptors
  }
}

public struct Sensory_Api_V1_Audio_AudioModelsNIOClient: Sensory_Api_V1_Audio_AudioModelsClientProtocol {
  public var channel: GRPCChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol?

  /// Creates a client for the sensory.api.v1.audio.AudioModels service.
  ///
  /// - Parameters:
  ///   - channel: `GRPCChannel` to the service host.
  ///   - defaultCallOptions: Options to use for each service call if the user doesn't provide them.
  ///   - interceptors: A factory providing interceptors for each RPC.
  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self.defaultCallOptions = defaultCallOptions
    self.interceptors = interceptors
  }
}

#if compiler(>=5.6)
/// Handles the retrieval and management of audio models
@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public protocol Sensory_Api_V1_Audio_AudioModelsAsyncClientProtocol: GRPCClient {
  static var serviceDescriptor: GRPCServiceDescriptor { get }
  var interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol? { get }

  func makeGetModelsCall(
    _ request: Sensory_Api_V1_Audio_GetModelsRequest,
    callOptions: CallOptions?
  ) -> GRPCAsyncUnaryCall<Sensory_Api_V1_Audio_GetModelsRequest, Sensory_Api_V1_Audio_GetModelsResponse>
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioModelsAsyncClientProtocol {
  public static var serviceDescriptor: GRPCServiceDescriptor {
    return Sensory_Api_V1_Audio_AudioModelsClientMetadata.serviceDescriptor
  }

  public var interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol? {
    return nil
  }

  public func makeGetModelsCall(
    _ request: Sensory_Api_V1_Audio_GetModelsRequest,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncUnaryCall<Sensory_Api_V1_Audio_GetModelsRequest, Sensory_Api_V1_Audio_GetModelsResponse> {
    return self.makeAsyncUnaryCall(
      path: Sensory_Api_V1_Audio_AudioModelsClientMetadata.Methods.getModels.path,
      request: request,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeGetModelsInterceptors() ?? []
    )
  }
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioModelsAsyncClientProtocol {
  public func getModels(
    _ request: Sensory_Api_V1_Audio_GetModelsRequest,
    callOptions: CallOptions? = nil
  ) async throws -> Sensory_Api_V1_Audio_GetModelsResponse {
    return try await self.performAsyncUnaryCall(
      path: Sensory_Api_V1_Audio_AudioModelsClientMetadata.Methods.getModels.path,
      request: request,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeGetModelsInterceptors() ?? []
    )
  }
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public struct Sensory_Api_V1_Audio_AudioModelsAsyncClient: Sensory_Api_V1_Audio_AudioModelsAsyncClientProtocol {
  public var channel: GRPCChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol?

  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self.defaultCallOptions = defaultCallOptions
    self.interceptors = interceptors
  }
}

#endif // compiler(>=5.6)

public protocol Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol: GRPCSendable {

  /// - Returns: Interceptors to use when invoking 'getModels'.
  func makeGetModelsInterceptors() -> [ClientInterceptor<Sensory_Api_V1_Audio_GetModelsRequest, Sensory_Api_V1_Audio_GetModelsResponse>]
}

public enum Sensory_Api_V1_Audio_AudioModelsClientMetadata {
  public static let serviceDescriptor = GRPCServiceDescriptor(
    name: "AudioModels",
    fullName: "sensory.api.v1.audio.AudioModels",
    methods: [
      Sensory_Api_V1_Audio_AudioModelsClientMetadata.Methods.getModels,
    ]
  )

  public enum Methods {
    public static let getModels = GRPCMethodDescriptor(
      name: "GetModels",
      path: "/sensory.api.v1.audio.AudioModels/GetModels",
      type: GRPCCallType.unary
    )
  }
}

#if compiler(>=5.6)
@available(swift, deprecated: 5.6)
extension Sensory_Api_V1_Audio_AudioModelsTestClient: @unchecked Sendable {}
#endif // compiler(>=5.6)

@available(swift, deprecated: 5.6, message: "Test clients are not Sendable but the 'GRPCClient' API requires clients to be Sendable. Using a localhost client and server is the recommended alternative.")
public final class Sensory_Api_V1_Audio_AudioModelsTestClient: Sensory_Api_V1_Audio_AudioModelsClientProtocol {
  private let fakeChannel: FakeChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol?

  public var channel: GRPCChannel {
    return self.fakeChannel
  }

  public init(
    fakeChannel: FakeChannel = FakeChannel(),
    defaultCallOptions callOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioModelsClientInterceptorFactoryProtocol? = nil
  ) {
    self.fakeChannel = fakeChannel
    self.defaultCallOptions = callOptions
    self.interceptors = interceptors
  }

  /// Make a unary response for the GetModels RPC. This must be called
  /// before calling 'getModels'. See also 'FakeUnaryResponse'.
  ///
  /// - Parameter requestHandler: a handler for request parts sent by the RPC.
  public func makeGetModelsResponseStream(
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_GetModelsRequest>) -> () = { _ in }
  ) -> FakeUnaryResponse<Sensory_Api_V1_Audio_GetModelsRequest, Sensory_Api_V1_Audio_GetModelsResponse> {
    return self.fakeChannel.makeFakeUnaryResponse(path: Sensory_Api_V1_Audio_AudioModelsClientMetadata.Methods.getModels.path, requestHandler: requestHandler)
  }

  public func enqueueGetModelsResponse(
    _ response: Sensory_Api_V1_Audio_GetModelsResponse,
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_GetModelsRequest>) -> () = { _ in }
  ) {
    let stream = self.makeGetModelsResponseStream(requestHandler)
    // This is the only operation on the stream; try! is fine.
    try! stream.sendMessage(response)
  }

  /// Returns true if there are response streams enqueued for 'GetModels'
  public var hasGetModelsResponsesRemaining: Bool {
    return self.fakeChannel.hasFakeResponseEnqueued(forPath: Sensory_Api_V1_Audio_AudioModelsClientMetadata.Methods.getModels.path)
  }
}

/// Handles all audio-related biometrics
///
/// Usage: instantiate `Sensory_Api_V1_Audio_AudioBiometricsClient`, then call methods of this protocol to make API calls.
public protocol Sensory_Api_V1_Audio_AudioBiometricsClientProtocol: GRPCClient {
  var serviceName: String { get }
  var interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol? { get }

  func createEnrollment(
    callOptions: CallOptions?,
    handler: @escaping (Sensory_Api_V1_Audio_CreateEnrollmentResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_CreateEnrollmentRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse>

  func authenticate(
    callOptions: CallOptions?,
    handler: @escaping (Sensory_Api_V1_Audio_AuthenticateResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_AuthenticateRequest, Sensory_Api_V1_Audio_AuthenticateResponse>
}

extension Sensory_Api_V1_Audio_AudioBiometricsClientProtocol {
  public var serviceName: String {
    return "sensory.api.v1.audio.AudioBiometrics"
  }

  /// Enrolls a user with a stream of audio. Streams a CreateEnrollmentResponse as the audio is processed.
  /// CreateEnrollment only supports biometric-enabled models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  ///
  /// Callers should use the `send` method on the returned object to send messages
  /// to the server. The caller should send an `.end` after the final message has been sent.
  ///
  /// - Parameters:
  ///   - callOptions: Call options.
  ///   - handler: A closure called when each response is received from the server.
  /// - Returns: A `ClientStreamingCall` with futures for the metadata and status.
  public func createEnrollment(
    callOptions: CallOptions? = nil,
    handler: @escaping (Sensory_Api_V1_Audio_CreateEnrollmentResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_CreateEnrollmentRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse> {
    return self.makeBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.createEnrollment.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeCreateEnrollmentInterceptors() ?? [],
      handler: handler
    )
  }

  /// Authenticates a user with a stream of audio against an existing enrollment.
  /// Streams an AuthenticateResponse as the audio is processed.
  /// Authenticate only supports biometric-enabled models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  ///
  /// Callers should use the `send` method on the returned object to send messages
  /// to the server. The caller should send an `.end` after the final message has been sent.
  ///
  /// - Parameters:
  ///   - callOptions: Call options.
  ///   - handler: A closure called when each response is received from the server.
  /// - Returns: A `ClientStreamingCall` with futures for the metadata and status.
  public func authenticate(
    callOptions: CallOptions? = nil,
    handler: @escaping (Sensory_Api_V1_Audio_AuthenticateResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_AuthenticateRequest, Sensory_Api_V1_Audio_AuthenticateResponse> {
    return self.makeBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.authenticate.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeAuthenticateInterceptors() ?? [],
      handler: handler
    )
  }
}

#if compiler(>=5.6)
@available(*, deprecated)
extension Sensory_Api_V1_Audio_AudioBiometricsClient: @unchecked Sendable {}
#endif // compiler(>=5.6)

@available(*, deprecated, renamed: "Sensory_Api_V1_Audio_AudioBiometricsNIOClient")
public final class Sensory_Api_V1_Audio_AudioBiometricsClient: Sensory_Api_V1_Audio_AudioBiometricsClientProtocol {
  private let lock = Lock()
  private var _defaultCallOptions: CallOptions
  private var _interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol?
  public let channel: GRPCChannel
  public var defaultCallOptions: CallOptions {
    get { self.lock.withLock { return self._defaultCallOptions } }
    set { self.lock.withLockVoid { self._defaultCallOptions = newValue } }
  }
  public var interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol? {
    get { self.lock.withLock { return self._interceptors } }
    set { self.lock.withLockVoid { self._interceptors = newValue } }
  }

  /// Creates a client for the sensory.api.v1.audio.AudioBiometrics service.
  ///
  /// - Parameters:
  ///   - channel: `GRPCChannel` to the service host.
  ///   - defaultCallOptions: Options to use for each service call if the user doesn't provide them.
  ///   - interceptors: A factory providing interceptors for each RPC.
  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self._defaultCallOptions = defaultCallOptions
    self._interceptors = interceptors
  }
}

public struct Sensory_Api_V1_Audio_AudioBiometricsNIOClient: Sensory_Api_V1_Audio_AudioBiometricsClientProtocol {
  public var channel: GRPCChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol?

  /// Creates a client for the sensory.api.v1.audio.AudioBiometrics service.
  ///
  /// - Parameters:
  ///   - channel: `GRPCChannel` to the service host.
  ///   - defaultCallOptions: Options to use for each service call if the user doesn't provide them.
  ///   - interceptors: A factory providing interceptors for each RPC.
  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self.defaultCallOptions = defaultCallOptions
    self.interceptors = interceptors
  }
}

#if compiler(>=5.6)
/// Handles all audio-related biometrics
@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public protocol Sensory_Api_V1_Audio_AudioBiometricsAsyncClientProtocol: GRPCClient {
  static var serviceDescriptor: GRPCServiceDescriptor { get }
  var interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol? { get }

  func makeCreateEnrollmentCall(
    callOptions: CallOptions?
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_CreateEnrollmentRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse>

  func makeAuthenticateCall(
    callOptions: CallOptions?
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_AuthenticateRequest, Sensory_Api_V1_Audio_AuthenticateResponse>
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioBiometricsAsyncClientProtocol {
  public static var serviceDescriptor: GRPCServiceDescriptor {
    return Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.serviceDescriptor
  }

  public var interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol? {
    return nil
  }

  public func makeCreateEnrollmentCall(
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_CreateEnrollmentRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse> {
    return self.makeAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.createEnrollment.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeCreateEnrollmentInterceptors() ?? []
    )
  }

  public func makeAuthenticateCall(
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_AuthenticateRequest, Sensory_Api_V1_Audio_AuthenticateResponse> {
    return self.makeAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.authenticate.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeAuthenticateInterceptors() ?? []
    )
  }
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioBiometricsAsyncClientProtocol {
  public func createEnrollment<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_CreateEnrollmentResponse> where RequestStream: Sequence, RequestStream.Element == Sensory_Api_V1_Audio_CreateEnrollmentRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.createEnrollment.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeCreateEnrollmentInterceptors() ?? []
    )
  }

  public func createEnrollment<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_CreateEnrollmentResponse> where RequestStream: AsyncSequence & Sendable, RequestStream.Element == Sensory_Api_V1_Audio_CreateEnrollmentRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.createEnrollment.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeCreateEnrollmentInterceptors() ?? []
    )
  }

  public func authenticate<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_AuthenticateResponse> where RequestStream: Sequence, RequestStream.Element == Sensory_Api_V1_Audio_AuthenticateRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.authenticate.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeAuthenticateInterceptors() ?? []
    )
  }

  public func authenticate<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_AuthenticateResponse> where RequestStream: AsyncSequence & Sendable, RequestStream.Element == Sensory_Api_V1_Audio_AuthenticateRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.authenticate.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeAuthenticateInterceptors() ?? []
    )
  }
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public struct Sensory_Api_V1_Audio_AudioBiometricsAsyncClient: Sensory_Api_V1_Audio_AudioBiometricsAsyncClientProtocol {
  public var channel: GRPCChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol?

  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self.defaultCallOptions = defaultCallOptions
    self.interceptors = interceptors
  }
}

#endif // compiler(>=5.6)

public protocol Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol: GRPCSendable {

  /// - Returns: Interceptors to use when invoking 'createEnrollment'.
  func makeCreateEnrollmentInterceptors() -> [ClientInterceptor<Sensory_Api_V1_Audio_CreateEnrollmentRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse>]

  /// - Returns: Interceptors to use when invoking 'authenticate'.
  func makeAuthenticateInterceptors() -> [ClientInterceptor<Sensory_Api_V1_Audio_AuthenticateRequest, Sensory_Api_V1_Audio_AuthenticateResponse>]
}

public enum Sensory_Api_V1_Audio_AudioBiometricsClientMetadata {
  public static let serviceDescriptor = GRPCServiceDescriptor(
    name: "AudioBiometrics",
    fullName: "sensory.api.v1.audio.AudioBiometrics",
    methods: [
      Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.createEnrollment,
      Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.authenticate,
    ]
  )

  public enum Methods {
    public static let createEnrollment = GRPCMethodDescriptor(
      name: "CreateEnrollment",
      path: "/sensory.api.v1.audio.AudioBiometrics/CreateEnrollment",
      type: GRPCCallType.bidirectionalStreaming
    )

    public static let authenticate = GRPCMethodDescriptor(
      name: "Authenticate",
      path: "/sensory.api.v1.audio.AudioBiometrics/Authenticate",
      type: GRPCCallType.bidirectionalStreaming
    )
  }
}

#if compiler(>=5.6)
@available(swift, deprecated: 5.6)
extension Sensory_Api_V1_Audio_AudioBiometricsTestClient: @unchecked Sendable {}
#endif // compiler(>=5.6)

@available(swift, deprecated: 5.6, message: "Test clients are not Sendable but the 'GRPCClient' API requires clients to be Sendable. Using a localhost client and server is the recommended alternative.")
public final class Sensory_Api_V1_Audio_AudioBiometricsTestClient: Sensory_Api_V1_Audio_AudioBiometricsClientProtocol {
  private let fakeChannel: FakeChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol?

  public var channel: GRPCChannel {
    return self.fakeChannel
  }

  public init(
    fakeChannel: FakeChannel = FakeChannel(),
    defaultCallOptions callOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioBiometricsClientInterceptorFactoryProtocol? = nil
  ) {
    self.fakeChannel = fakeChannel
    self.defaultCallOptions = callOptions
    self.interceptors = interceptors
  }

  /// Make a streaming response for the CreateEnrollment RPC. This must be called
  /// before calling 'createEnrollment'. See also 'FakeStreamingResponse'.
  ///
  /// - Parameter requestHandler: a handler for request parts sent by the RPC.
  public func makeCreateEnrollmentResponseStream(
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_CreateEnrollmentRequest>) -> () = { _ in }
  ) -> FakeStreamingResponse<Sensory_Api_V1_Audio_CreateEnrollmentRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse> {
    return self.fakeChannel.makeFakeStreamingResponse(path: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.createEnrollment.path, requestHandler: requestHandler)
  }

  public func enqueueCreateEnrollmentResponses(
    _ responses: [Sensory_Api_V1_Audio_CreateEnrollmentResponse],
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_CreateEnrollmentRequest>) -> () = { _ in }
  ) {
    let stream = self.makeCreateEnrollmentResponseStream(requestHandler)
    // These are the only operation on the stream; try! is fine.
    responses.forEach { try! stream.sendMessage($0) }
    try! stream.sendEnd()
  }

  /// Returns true if there are response streams enqueued for 'CreateEnrollment'
  public var hasCreateEnrollmentResponsesRemaining: Bool {
    return self.fakeChannel.hasFakeResponseEnqueued(forPath: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.createEnrollment.path)
  }

  /// Make a streaming response for the Authenticate RPC. This must be called
  /// before calling 'authenticate'. See also 'FakeStreamingResponse'.
  ///
  /// - Parameter requestHandler: a handler for request parts sent by the RPC.
  public func makeAuthenticateResponseStream(
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_AuthenticateRequest>) -> () = { _ in }
  ) -> FakeStreamingResponse<Sensory_Api_V1_Audio_AuthenticateRequest, Sensory_Api_V1_Audio_AuthenticateResponse> {
    return self.fakeChannel.makeFakeStreamingResponse(path: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.authenticate.path, requestHandler: requestHandler)
  }

  public func enqueueAuthenticateResponses(
    _ responses: [Sensory_Api_V1_Audio_AuthenticateResponse],
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_AuthenticateRequest>) -> () = { _ in }
  ) {
    let stream = self.makeAuthenticateResponseStream(requestHandler)
    // These are the only operation on the stream; try! is fine.
    responses.forEach { try! stream.sendMessage($0) }
    try! stream.sendEnd()
  }

  /// Returns true if there are response streams enqueued for 'Authenticate'
  public var hasAuthenticateResponsesRemaining: Bool {
    return self.fakeChannel.hasFakeResponseEnqueued(forPath: Sensory_Api_V1_Audio_AudioBiometricsClientMetadata.Methods.authenticate.path)
  }
}

/// Handles all audio event processing
///
/// Usage: instantiate `Sensory_Api_V1_Audio_AudioEventsClient`, then call methods of this protocol to make API calls.
public protocol Sensory_Api_V1_Audio_AudioEventsClientProtocol: GRPCClient {
  var serviceName: String { get }
  var interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol? { get }

  func validateEvent(
    callOptions: CallOptions?,
    handler: @escaping (Sensory_Api_V1_Audio_ValidateEventResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_ValidateEventRequest, Sensory_Api_V1_Audio_ValidateEventResponse>

  func createEnrolledEvent(
    callOptions: CallOptions?,
    handler: @escaping (Sensory_Api_V1_Audio_CreateEnrollmentResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_CreateEnrolledEventRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse>

  func validateEnrolledEvent(
    callOptions: CallOptions?,
    handler: @escaping (Sensory_Api_V1_Audio_ValidateEnrolledEventResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest, Sensory_Api_V1_Audio_ValidateEnrolledEventResponse>
}

extension Sensory_Api_V1_Audio_AudioEventsClientProtocol {
  public var serviceName: String {
    return "sensory.api.v1.audio.AudioEvents"
  }

  /// Validates a phrase or sound with a stream of audio.
  /// Streams a ValidateEventResponse as the audio is processed.
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  ///
  /// Callers should use the `send` method on the returned object to send messages
  /// to the server. The caller should send an `.end` after the final message has been sent.
  ///
  /// - Parameters:
  ///   - callOptions: Call options.
  ///   - handler: A closure called when each response is received from the server.
  /// - Returns: A `ClientStreamingCall` with futures for the metadata and status.
  public func validateEvent(
    callOptions: CallOptions? = nil,
    handler: @escaping (Sensory_Api_V1_Audio_ValidateEventResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_ValidateEventRequest, Sensory_Api_V1_Audio_ValidateEventResponse> {
    return self.makeBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEvent.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeValidateEventInterceptors() ?? [],
      handler: handler
    )
  }

  /// Enrolls a sound or voice. Streams a CreateEnrollmentResponse as the audio is processed.
  /// CreateEnrollment supports all enrollable models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  ///
  /// Callers should use the `send` method on the returned object to send messages
  /// to the server. The caller should send an `.end` after the final message has been sent.
  ///
  /// - Parameters:
  ///   - callOptions: Call options.
  ///   - handler: A closure called when each response is received from the server.
  /// - Returns: A `ClientStreamingCall` with futures for the metadata and status.
  public func createEnrolledEvent(
    callOptions: CallOptions? = nil,
    handler: @escaping (Sensory_Api_V1_Audio_CreateEnrollmentResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_CreateEnrolledEventRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse> {
    return self.makeBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.createEnrolledEvent.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeCreateEnrolledEventInterceptors() ?? [],
      handler: handler
    )
  }

  /// Authenticates a sound or voice. Streams a ValidateEventResponse as the audio is processed.
  /// ValidateEnrolledEvent supports all enrollable models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  ///
  /// Callers should use the `send` method on the returned object to send messages
  /// to the server. The caller should send an `.end` after the final message has been sent.
  ///
  /// - Parameters:
  ///   - callOptions: Call options.
  ///   - handler: A closure called when each response is received from the server.
  /// - Returns: A `ClientStreamingCall` with futures for the metadata and status.
  public func validateEnrolledEvent(
    callOptions: CallOptions? = nil,
    handler: @escaping (Sensory_Api_V1_Audio_ValidateEnrolledEventResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest, Sensory_Api_V1_Audio_ValidateEnrolledEventResponse> {
    return self.makeBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEnrolledEvent.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeValidateEnrolledEventInterceptors() ?? [],
      handler: handler
    )
  }
}

#if compiler(>=5.6)
@available(*, deprecated)
extension Sensory_Api_V1_Audio_AudioEventsClient: @unchecked Sendable {}
#endif // compiler(>=5.6)

@available(*, deprecated, renamed: "Sensory_Api_V1_Audio_AudioEventsNIOClient")
public final class Sensory_Api_V1_Audio_AudioEventsClient: Sensory_Api_V1_Audio_AudioEventsClientProtocol {
  private let lock = Lock()
  private var _defaultCallOptions: CallOptions
  private var _interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol?
  public let channel: GRPCChannel
  public var defaultCallOptions: CallOptions {
    get { self.lock.withLock { return self._defaultCallOptions } }
    set { self.lock.withLockVoid { self._defaultCallOptions = newValue } }
  }
  public var interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol? {
    get { self.lock.withLock { return self._interceptors } }
    set { self.lock.withLockVoid { self._interceptors = newValue } }
  }

  /// Creates a client for the sensory.api.v1.audio.AudioEvents service.
  ///
  /// - Parameters:
  ///   - channel: `GRPCChannel` to the service host.
  ///   - defaultCallOptions: Options to use for each service call if the user doesn't provide them.
  ///   - interceptors: A factory providing interceptors for each RPC.
  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self._defaultCallOptions = defaultCallOptions
    self._interceptors = interceptors
  }
}

public struct Sensory_Api_V1_Audio_AudioEventsNIOClient: Sensory_Api_V1_Audio_AudioEventsClientProtocol {
  public var channel: GRPCChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol?

  /// Creates a client for the sensory.api.v1.audio.AudioEvents service.
  ///
  /// - Parameters:
  ///   - channel: `GRPCChannel` to the service host.
  ///   - defaultCallOptions: Options to use for each service call if the user doesn't provide them.
  ///   - interceptors: A factory providing interceptors for each RPC.
  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self.defaultCallOptions = defaultCallOptions
    self.interceptors = interceptors
  }
}

#if compiler(>=5.6)
/// Handles all audio event processing
@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public protocol Sensory_Api_V1_Audio_AudioEventsAsyncClientProtocol: GRPCClient {
  static var serviceDescriptor: GRPCServiceDescriptor { get }
  var interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol? { get }

  func makeValidateEventCall(
    callOptions: CallOptions?
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_ValidateEventRequest, Sensory_Api_V1_Audio_ValidateEventResponse>

  func makeCreateEnrolledEventCall(
    callOptions: CallOptions?
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_CreateEnrolledEventRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse>

  func makeValidateEnrolledEventCall(
    callOptions: CallOptions?
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest, Sensory_Api_V1_Audio_ValidateEnrolledEventResponse>
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioEventsAsyncClientProtocol {
  public static var serviceDescriptor: GRPCServiceDescriptor {
    return Sensory_Api_V1_Audio_AudioEventsClientMetadata.serviceDescriptor
  }

  public var interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol? {
    return nil
  }

  public func makeValidateEventCall(
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_ValidateEventRequest, Sensory_Api_V1_Audio_ValidateEventResponse> {
    return self.makeAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEvent.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeValidateEventInterceptors() ?? []
    )
  }

  public func makeCreateEnrolledEventCall(
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_CreateEnrolledEventRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse> {
    return self.makeAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.createEnrolledEvent.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeCreateEnrolledEventInterceptors() ?? []
    )
  }

  public func makeValidateEnrolledEventCall(
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest, Sensory_Api_V1_Audio_ValidateEnrolledEventResponse> {
    return self.makeAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEnrolledEvent.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeValidateEnrolledEventInterceptors() ?? []
    )
  }
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioEventsAsyncClientProtocol {
  public func validateEvent<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_ValidateEventResponse> where RequestStream: Sequence, RequestStream.Element == Sensory_Api_V1_Audio_ValidateEventRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEvent.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeValidateEventInterceptors() ?? []
    )
  }

  public func validateEvent<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_ValidateEventResponse> where RequestStream: AsyncSequence & Sendable, RequestStream.Element == Sensory_Api_V1_Audio_ValidateEventRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEvent.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeValidateEventInterceptors() ?? []
    )
  }

  public func createEnrolledEvent<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_CreateEnrollmentResponse> where RequestStream: Sequence, RequestStream.Element == Sensory_Api_V1_Audio_CreateEnrolledEventRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.createEnrolledEvent.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeCreateEnrolledEventInterceptors() ?? []
    )
  }

  public func createEnrolledEvent<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_CreateEnrollmentResponse> where RequestStream: AsyncSequence & Sendable, RequestStream.Element == Sensory_Api_V1_Audio_CreateEnrolledEventRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.createEnrolledEvent.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeCreateEnrolledEventInterceptors() ?? []
    )
  }

  public func validateEnrolledEvent<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_ValidateEnrolledEventResponse> where RequestStream: Sequence, RequestStream.Element == Sensory_Api_V1_Audio_ValidateEnrolledEventRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEnrolledEvent.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeValidateEnrolledEventInterceptors() ?? []
    )
  }

  public func validateEnrolledEvent<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_ValidateEnrolledEventResponse> where RequestStream: AsyncSequence & Sendable, RequestStream.Element == Sensory_Api_V1_Audio_ValidateEnrolledEventRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEnrolledEvent.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeValidateEnrolledEventInterceptors() ?? []
    )
  }
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public struct Sensory_Api_V1_Audio_AudioEventsAsyncClient: Sensory_Api_V1_Audio_AudioEventsAsyncClientProtocol {
  public var channel: GRPCChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol?

  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self.defaultCallOptions = defaultCallOptions
    self.interceptors = interceptors
  }
}

#endif // compiler(>=5.6)

public protocol Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol: GRPCSendable {

  /// - Returns: Interceptors to use when invoking 'validateEvent'.
  func makeValidateEventInterceptors() -> [ClientInterceptor<Sensory_Api_V1_Audio_ValidateEventRequest, Sensory_Api_V1_Audio_ValidateEventResponse>]

  /// - Returns: Interceptors to use when invoking 'createEnrolledEvent'.
  func makeCreateEnrolledEventInterceptors() -> [ClientInterceptor<Sensory_Api_V1_Audio_CreateEnrolledEventRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse>]

  /// - Returns: Interceptors to use when invoking 'validateEnrolledEvent'.
  func makeValidateEnrolledEventInterceptors() -> [ClientInterceptor<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest, Sensory_Api_V1_Audio_ValidateEnrolledEventResponse>]
}

public enum Sensory_Api_V1_Audio_AudioEventsClientMetadata {
  public static let serviceDescriptor = GRPCServiceDescriptor(
    name: "AudioEvents",
    fullName: "sensory.api.v1.audio.AudioEvents",
    methods: [
      Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEvent,
      Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.createEnrolledEvent,
      Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEnrolledEvent,
    ]
  )

  public enum Methods {
    public static let validateEvent = GRPCMethodDescriptor(
      name: "ValidateEvent",
      path: "/sensory.api.v1.audio.AudioEvents/ValidateEvent",
      type: GRPCCallType.bidirectionalStreaming
    )

    public static let createEnrolledEvent = GRPCMethodDescriptor(
      name: "CreateEnrolledEvent",
      path: "/sensory.api.v1.audio.AudioEvents/CreateEnrolledEvent",
      type: GRPCCallType.bidirectionalStreaming
    )

    public static let validateEnrolledEvent = GRPCMethodDescriptor(
      name: "ValidateEnrolledEvent",
      path: "/sensory.api.v1.audio.AudioEvents/ValidateEnrolledEvent",
      type: GRPCCallType.bidirectionalStreaming
    )
  }
}

#if compiler(>=5.6)
@available(swift, deprecated: 5.6)
extension Sensory_Api_V1_Audio_AudioEventsTestClient: @unchecked Sendable {}
#endif // compiler(>=5.6)

@available(swift, deprecated: 5.6, message: "Test clients are not Sendable but the 'GRPCClient' API requires clients to be Sendable. Using a localhost client and server is the recommended alternative.")
public final class Sensory_Api_V1_Audio_AudioEventsTestClient: Sensory_Api_V1_Audio_AudioEventsClientProtocol {
  private let fakeChannel: FakeChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol?

  public var channel: GRPCChannel {
    return self.fakeChannel
  }

  public init(
    fakeChannel: FakeChannel = FakeChannel(),
    defaultCallOptions callOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioEventsClientInterceptorFactoryProtocol? = nil
  ) {
    self.fakeChannel = fakeChannel
    self.defaultCallOptions = callOptions
    self.interceptors = interceptors
  }

  /// Make a streaming response for the ValidateEvent RPC. This must be called
  /// before calling 'validateEvent'. See also 'FakeStreamingResponse'.
  ///
  /// - Parameter requestHandler: a handler for request parts sent by the RPC.
  public func makeValidateEventResponseStream(
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_ValidateEventRequest>) -> () = { _ in }
  ) -> FakeStreamingResponse<Sensory_Api_V1_Audio_ValidateEventRequest, Sensory_Api_V1_Audio_ValidateEventResponse> {
    return self.fakeChannel.makeFakeStreamingResponse(path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEvent.path, requestHandler: requestHandler)
  }

  public func enqueueValidateEventResponses(
    _ responses: [Sensory_Api_V1_Audio_ValidateEventResponse],
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_ValidateEventRequest>) -> () = { _ in }
  ) {
    let stream = self.makeValidateEventResponseStream(requestHandler)
    // These are the only operation on the stream; try! is fine.
    responses.forEach { try! stream.sendMessage($0) }
    try! stream.sendEnd()
  }

  /// Returns true if there are response streams enqueued for 'ValidateEvent'
  public var hasValidateEventResponsesRemaining: Bool {
    return self.fakeChannel.hasFakeResponseEnqueued(forPath: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEvent.path)
  }

  /// Make a streaming response for the CreateEnrolledEvent RPC. This must be called
  /// before calling 'createEnrolledEvent'. See also 'FakeStreamingResponse'.
  ///
  /// - Parameter requestHandler: a handler for request parts sent by the RPC.
  public func makeCreateEnrolledEventResponseStream(
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_CreateEnrolledEventRequest>) -> () = { _ in }
  ) -> FakeStreamingResponse<Sensory_Api_V1_Audio_CreateEnrolledEventRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse> {
    return self.fakeChannel.makeFakeStreamingResponse(path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.createEnrolledEvent.path, requestHandler: requestHandler)
  }

  public func enqueueCreateEnrolledEventResponses(
    _ responses: [Sensory_Api_V1_Audio_CreateEnrollmentResponse],
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_CreateEnrolledEventRequest>) -> () = { _ in }
  ) {
    let stream = self.makeCreateEnrolledEventResponseStream(requestHandler)
    // These are the only operation on the stream; try! is fine.
    responses.forEach { try! stream.sendMessage($0) }
    try! stream.sendEnd()
  }

  /// Returns true if there are response streams enqueued for 'CreateEnrolledEvent'
  public var hasCreateEnrolledEventResponsesRemaining: Bool {
    return self.fakeChannel.hasFakeResponseEnqueued(forPath: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.createEnrolledEvent.path)
  }

  /// Make a streaming response for the ValidateEnrolledEvent RPC. This must be called
  /// before calling 'validateEnrolledEvent'. See also 'FakeStreamingResponse'.
  ///
  /// - Parameter requestHandler: a handler for request parts sent by the RPC.
  public func makeValidateEnrolledEventResponseStream(
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest>) -> () = { _ in }
  ) -> FakeStreamingResponse<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest, Sensory_Api_V1_Audio_ValidateEnrolledEventResponse> {
    return self.fakeChannel.makeFakeStreamingResponse(path: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEnrolledEvent.path, requestHandler: requestHandler)
  }

  public func enqueueValidateEnrolledEventResponses(
    _ responses: [Sensory_Api_V1_Audio_ValidateEnrolledEventResponse],
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest>) -> () = { _ in }
  ) {
    let stream = self.makeValidateEnrolledEventResponseStream(requestHandler)
    // These are the only operation on the stream; try! is fine.
    responses.forEach { try! stream.sendMessage($0) }
    try! stream.sendEnd()
  }

  /// Returns true if there are response streams enqueued for 'ValidateEnrolledEvent'
  public var hasValidateEnrolledEventResponsesRemaining: Bool {
    return self.fakeChannel.hasFakeResponseEnqueued(forPath: Sensory_Api_V1_Audio_AudioEventsClientMetadata.Methods.validateEnrolledEvent.path)
  }
}

/// Handles all audio transcriptions
///
/// Usage: instantiate `Sensory_Api_V1_Audio_AudioTranscriptionsClient`, then call methods of this protocol to make API calls.
public protocol Sensory_Api_V1_Audio_AudioTranscriptionsClientProtocol: GRPCClient {
  var serviceName: String { get }
  var interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol? { get }

  func transcribe(
    callOptions: CallOptions?,
    handler: @escaping (Sensory_Api_V1_Audio_TranscribeResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_TranscribeRequest, Sensory_Api_V1_Audio_TranscribeResponse>
}

extension Sensory_Api_V1_Audio_AudioTranscriptionsClientProtocol {
  public var serviceName: String {
    return "sensory.api.v1.audio.AudioTranscriptions"
  }

  /// Transcribes voice into text.
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  ///
  /// Callers should use the `send` method on the returned object to send messages
  /// to the server. The caller should send an `.end` after the final message has been sent.
  ///
  /// - Parameters:
  ///   - callOptions: Call options.
  ///   - handler: A closure called when each response is received from the server.
  /// - Returns: A `ClientStreamingCall` with futures for the metadata and status.
  public func transcribe(
    callOptions: CallOptions? = nil,
    handler: @escaping (Sensory_Api_V1_Audio_TranscribeResponse) -> Void
  ) -> BidirectionalStreamingCall<Sensory_Api_V1_Audio_TranscribeRequest, Sensory_Api_V1_Audio_TranscribeResponse> {
    return self.makeBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioTranscriptionsClientMetadata.Methods.transcribe.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeTranscribeInterceptors() ?? [],
      handler: handler
    )
  }
}

#if compiler(>=5.6)
@available(*, deprecated)
extension Sensory_Api_V1_Audio_AudioTranscriptionsClient: @unchecked Sendable {}
#endif // compiler(>=5.6)

@available(*, deprecated, renamed: "Sensory_Api_V1_Audio_AudioTranscriptionsNIOClient")
public final class Sensory_Api_V1_Audio_AudioTranscriptionsClient: Sensory_Api_V1_Audio_AudioTranscriptionsClientProtocol {
  private let lock = Lock()
  private var _defaultCallOptions: CallOptions
  private var _interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol?
  public let channel: GRPCChannel
  public var defaultCallOptions: CallOptions {
    get { self.lock.withLock { return self._defaultCallOptions } }
    set { self.lock.withLockVoid { self._defaultCallOptions = newValue } }
  }
  public var interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol? {
    get { self.lock.withLock { return self._interceptors } }
    set { self.lock.withLockVoid { self._interceptors = newValue } }
  }

  /// Creates a client for the sensory.api.v1.audio.AudioTranscriptions service.
  ///
  /// - Parameters:
  ///   - channel: `GRPCChannel` to the service host.
  ///   - defaultCallOptions: Options to use for each service call if the user doesn't provide them.
  ///   - interceptors: A factory providing interceptors for each RPC.
  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self._defaultCallOptions = defaultCallOptions
    self._interceptors = interceptors
  }
}

public struct Sensory_Api_V1_Audio_AudioTranscriptionsNIOClient: Sensory_Api_V1_Audio_AudioTranscriptionsClientProtocol {
  public var channel: GRPCChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol?

  /// Creates a client for the sensory.api.v1.audio.AudioTranscriptions service.
  ///
  /// - Parameters:
  ///   - channel: `GRPCChannel` to the service host.
  ///   - defaultCallOptions: Options to use for each service call if the user doesn't provide them.
  ///   - interceptors: A factory providing interceptors for each RPC.
  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self.defaultCallOptions = defaultCallOptions
    self.interceptors = interceptors
  }
}

#if compiler(>=5.6)
/// Handles all audio transcriptions
@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public protocol Sensory_Api_V1_Audio_AudioTranscriptionsAsyncClientProtocol: GRPCClient {
  static var serviceDescriptor: GRPCServiceDescriptor { get }
  var interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol? { get }

  func makeTranscribeCall(
    callOptions: CallOptions?
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_TranscribeRequest, Sensory_Api_V1_Audio_TranscribeResponse>
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioTranscriptionsAsyncClientProtocol {
  public static var serviceDescriptor: GRPCServiceDescriptor {
    return Sensory_Api_V1_Audio_AudioTranscriptionsClientMetadata.serviceDescriptor
  }

  public var interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol? {
    return nil
  }

  public func makeTranscribeCall(
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncBidirectionalStreamingCall<Sensory_Api_V1_Audio_TranscribeRequest, Sensory_Api_V1_Audio_TranscribeResponse> {
    return self.makeAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioTranscriptionsClientMetadata.Methods.transcribe.path,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeTranscribeInterceptors() ?? []
    )
  }
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioTranscriptionsAsyncClientProtocol {
  public func transcribe<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_TranscribeResponse> where RequestStream: Sequence, RequestStream.Element == Sensory_Api_V1_Audio_TranscribeRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioTranscriptionsClientMetadata.Methods.transcribe.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeTranscribeInterceptors() ?? []
    )
  }

  public func transcribe<RequestStream>(
    _ requests: RequestStream,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_TranscribeResponse> where RequestStream: AsyncSequence & Sendable, RequestStream.Element == Sensory_Api_V1_Audio_TranscribeRequest {
    return self.performAsyncBidirectionalStreamingCall(
      path: Sensory_Api_V1_Audio_AudioTranscriptionsClientMetadata.Methods.transcribe.path,
      requests: requests,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeTranscribeInterceptors() ?? []
    )
  }
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public struct Sensory_Api_V1_Audio_AudioTranscriptionsAsyncClient: Sensory_Api_V1_Audio_AudioTranscriptionsAsyncClientProtocol {
  public var channel: GRPCChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol?

  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self.defaultCallOptions = defaultCallOptions
    self.interceptors = interceptors
  }
}

#endif // compiler(>=5.6)

public protocol Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol: GRPCSendable {

  /// - Returns: Interceptors to use when invoking 'transcribe'.
  func makeTranscribeInterceptors() -> [ClientInterceptor<Sensory_Api_V1_Audio_TranscribeRequest, Sensory_Api_V1_Audio_TranscribeResponse>]
}

public enum Sensory_Api_V1_Audio_AudioTranscriptionsClientMetadata {
  public static let serviceDescriptor = GRPCServiceDescriptor(
    name: "AudioTranscriptions",
    fullName: "sensory.api.v1.audio.AudioTranscriptions",
    methods: [
      Sensory_Api_V1_Audio_AudioTranscriptionsClientMetadata.Methods.transcribe,
    ]
  )

  public enum Methods {
    public static let transcribe = GRPCMethodDescriptor(
      name: "Transcribe",
      path: "/sensory.api.v1.audio.AudioTranscriptions/Transcribe",
      type: GRPCCallType.bidirectionalStreaming
    )
  }
}

#if compiler(>=5.6)
@available(swift, deprecated: 5.6)
extension Sensory_Api_V1_Audio_AudioTranscriptionsTestClient: @unchecked Sendable {}
#endif // compiler(>=5.6)

@available(swift, deprecated: 5.6, message: "Test clients are not Sendable but the 'GRPCClient' API requires clients to be Sendable. Using a localhost client and server is the recommended alternative.")
public final class Sensory_Api_V1_Audio_AudioTranscriptionsTestClient: Sensory_Api_V1_Audio_AudioTranscriptionsClientProtocol {
  private let fakeChannel: FakeChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol?

  public var channel: GRPCChannel {
    return self.fakeChannel
  }

  public init(
    fakeChannel: FakeChannel = FakeChannel(),
    defaultCallOptions callOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsClientInterceptorFactoryProtocol? = nil
  ) {
    self.fakeChannel = fakeChannel
    self.defaultCallOptions = callOptions
    self.interceptors = interceptors
  }

  /// Make a streaming response for the Transcribe RPC. This must be called
  /// before calling 'transcribe'. See also 'FakeStreamingResponse'.
  ///
  /// - Parameter requestHandler: a handler for request parts sent by the RPC.
  public func makeTranscribeResponseStream(
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_TranscribeRequest>) -> () = { _ in }
  ) -> FakeStreamingResponse<Sensory_Api_V1_Audio_TranscribeRequest, Sensory_Api_V1_Audio_TranscribeResponse> {
    return self.fakeChannel.makeFakeStreamingResponse(path: Sensory_Api_V1_Audio_AudioTranscriptionsClientMetadata.Methods.transcribe.path, requestHandler: requestHandler)
  }

  public func enqueueTranscribeResponses(
    _ responses: [Sensory_Api_V1_Audio_TranscribeResponse],
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_TranscribeRequest>) -> () = { _ in }
  ) {
    let stream = self.makeTranscribeResponseStream(requestHandler)
    // These are the only operation on the stream; try! is fine.
    responses.forEach { try! stream.sendMessage($0) }
    try! stream.sendEnd()
  }

  /// Returns true if there are response streams enqueued for 'Transcribe'
  public var hasTranscribeResponsesRemaining: Bool {
    return self.fakeChannel.hasFakeResponseEnqueued(forPath: Sensory_Api_V1_Audio_AudioTranscriptionsClientMetadata.Methods.transcribe.path)
  }
}

/// Handles synthesizing audio from text
///
/// Usage: instantiate `Sensory_Api_V1_Audio_AudioSynthesisClient`, then call methods of this protocol to make API calls.
public protocol Sensory_Api_V1_Audio_AudioSynthesisClientProtocol: GRPCClient {
  var serviceName: String { get }
  var interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol? { get }

  func synthesizeSpeech(
    _ request: Sensory_Api_V1_Audio_SynthesizeSpeechRequest,
    callOptions: CallOptions?,
    handler: @escaping (Sensory_Api_V1_Audio_SynthesizeSpeechResponse) -> Void
  ) -> ServerStreamingCall<Sensory_Api_V1_Audio_SynthesizeSpeechRequest, Sensory_Api_V1_Audio_SynthesizeSpeechResponse>
}

extension Sensory_Api_V1_Audio_AudioSynthesisClientProtocol {
  public var serviceName: String {
    return "sensory.api.v1.audio.AudioSynthesis"
  }

  /// Synthesizes speech from text
  /// Authorization metadata is required {"authorization": "Bearer <TOKNE>"}
  ///
  /// - Parameters:
  ///   - request: Request to send to SynthesizeSpeech.
  ///   - callOptions: Call options.
  ///   - handler: A closure called when each response is received from the server.
  /// - Returns: A `ServerStreamingCall` with futures for the metadata and status.
  public func synthesizeSpeech(
    _ request: Sensory_Api_V1_Audio_SynthesizeSpeechRequest,
    callOptions: CallOptions? = nil,
    handler: @escaping (Sensory_Api_V1_Audio_SynthesizeSpeechResponse) -> Void
  ) -> ServerStreamingCall<Sensory_Api_V1_Audio_SynthesizeSpeechRequest, Sensory_Api_V1_Audio_SynthesizeSpeechResponse> {
    return self.makeServerStreamingCall(
      path: Sensory_Api_V1_Audio_AudioSynthesisClientMetadata.Methods.synthesizeSpeech.path,
      request: request,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeSynthesizeSpeechInterceptors() ?? [],
      handler: handler
    )
  }
}

#if compiler(>=5.6)
@available(*, deprecated)
extension Sensory_Api_V1_Audio_AudioSynthesisClient: @unchecked Sendable {}
#endif // compiler(>=5.6)

@available(*, deprecated, renamed: "Sensory_Api_V1_Audio_AudioSynthesisNIOClient")
public final class Sensory_Api_V1_Audio_AudioSynthesisClient: Sensory_Api_V1_Audio_AudioSynthesisClientProtocol {
  private let lock = Lock()
  private var _defaultCallOptions: CallOptions
  private var _interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol?
  public let channel: GRPCChannel
  public var defaultCallOptions: CallOptions {
    get { self.lock.withLock { return self._defaultCallOptions } }
    set { self.lock.withLockVoid { self._defaultCallOptions = newValue } }
  }
  public var interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol? {
    get { self.lock.withLock { return self._interceptors } }
    set { self.lock.withLockVoid { self._interceptors = newValue } }
  }

  /// Creates a client for the sensory.api.v1.audio.AudioSynthesis service.
  ///
  /// - Parameters:
  ///   - channel: `GRPCChannel` to the service host.
  ///   - defaultCallOptions: Options to use for each service call if the user doesn't provide them.
  ///   - interceptors: A factory providing interceptors for each RPC.
  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self._defaultCallOptions = defaultCallOptions
    self._interceptors = interceptors
  }
}

public struct Sensory_Api_V1_Audio_AudioSynthesisNIOClient: Sensory_Api_V1_Audio_AudioSynthesisClientProtocol {
  public var channel: GRPCChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol?

  /// Creates a client for the sensory.api.v1.audio.AudioSynthesis service.
  ///
  /// - Parameters:
  ///   - channel: `GRPCChannel` to the service host.
  ///   - defaultCallOptions: Options to use for each service call if the user doesn't provide them.
  ///   - interceptors: A factory providing interceptors for each RPC.
  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self.defaultCallOptions = defaultCallOptions
    self.interceptors = interceptors
  }
}

#if compiler(>=5.6)
/// Handles synthesizing audio from text
@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public protocol Sensory_Api_V1_Audio_AudioSynthesisAsyncClientProtocol: GRPCClient {
  static var serviceDescriptor: GRPCServiceDescriptor { get }
  var interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol? { get }

  func makeSynthesizeSpeechCall(
    _ request: Sensory_Api_V1_Audio_SynthesizeSpeechRequest,
    callOptions: CallOptions?
  ) -> GRPCAsyncServerStreamingCall<Sensory_Api_V1_Audio_SynthesizeSpeechRequest, Sensory_Api_V1_Audio_SynthesizeSpeechResponse>
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioSynthesisAsyncClientProtocol {
  public static var serviceDescriptor: GRPCServiceDescriptor {
    return Sensory_Api_V1_Audio_AudioSynthesisClientMetadata.serviceDescriptor
  }

  public var interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol? {
    return nil
  }

  public func makeSynthesizeSpeechCall(
    _ request: Sensory_Api_V1_Audio_SynthesizeSpeechRequest,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncServerStreamingCall<Sensory_Api_V1_Audio_SynthesizeSpeechRequest, Sensory_Api_V1_Audio_SynthesizeSpeechResponse> {
    return self.makeAsyncServerStreamingCall(
      path: Sensory_Api_V1_Audio_AudioSynthesisClientMetadata.Methods.synthesizeSpeech.path,
      request: request,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeSynthesizeSpeechInterceptors() ?? []
    )
  }
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioSynthesisAsyncClientProtocol {
  public func synthesizeSpeech(
    _ request: Sensory_Api_V1_Audio_SynthesizeSpeechRequest,
    callOptions: CallOptions? = nil
  ) -> GRPCAsyncResponseStream<Sensory_Api_V1_Audio_SynthesizeSpeechResponse> {
    return self.performAsyncServerStreamingCall(
      path: Sensory_Api_V1_Audio_AudioSynthesisClientMetadata.Methods.synthesizeSpeech.path,
      request: request,
      callOptions: callOptions ?? self.defaultCallOptions,
      interceptors: self.interceptors?.makeSynthesizeSpeechInterceptors() ?? []
    )
  }
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public struct Sensory_Api_V1_Audio_AudioSynthesisAsyncClient: Sensory_Api_V1_Audio_AudioSynthesisAsyncClientProtocol {
  public var channel: GRPCChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol?

  public init(
    channel: GRPCChannel,
    defaultCallOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol? = nil
  ) {
    self.channel = channel
    self.defaultCallOptions = defaultCallOptions
    self.interceptors = interceptors
  }
}

#endif // compiler(>=5.6)

public protocol Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol: GRPCSendable {

  /// - Returns: Interceptors to use when invoking 'synthesizeSpeech'.
  func makeSynthesizeSpeechInterceptors() -> [ClientInterceptor<Sensory_Api_V1_Audio_SynthesizeSpeechRequest, Sensory_Api_V1_Audio_SynthesizeSpeechResponse>]
}

public enum Sensory_Api_V1_Audio_AudioSynthesisClientMetadata {
  public static let serviceDescriptor = GRPCServiceDescriptor(
    name: "AudioSynthesis",
    fullName: "sensory.api.v1.audio.AudioSynthesis",
    methods: [
      Sensory_Api_V1_Audio_AudioSynthesisClientMetadata.Methods.synthesizeSpeech,
    ]
  )

  public enum Methods {
    public static let synthesizeSpeech = GRPCMethodDescriptor(
      name: "SynthesizeSpeech",
      path: "/sensory.api.v1.audio.AudioSynthesis/SynthesizeSpeech",
      type: GRPCCallType.serverStreaming
    )
  }
}

#if compiler(>=5.6)
@available(swift, deprecated: 5.6)
extension Sensory_Api_V1_Audio_AudioSynthesisTestClient: @unchecked Sendable {}
#endif // compiler(>=5.6)

@available(swift, deprecated: 5.6, message: "Test clients are not Sendable but the 'GRPCClient' API requires clients to be Sendable. Using a localhost client and server is the recommended alternative.")
public final class Sensory_Api_V1_Audio_AudioSynthesisTestClient: Sensory_Api_V1_Audio_AudioSynthesisClientProtocol {
  private let fakeChannel: FakeChannel
  public var defaultCallOptions: CallOptions
  public var interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol?

  public var channel: GRPCChannel {
    return self.fakeChannel
  }

  public init(
    fakeChannel: FakeChannel = FakeChannel(),
    defaultCallOptions callOptions: CallOptions = CallOptions(),
    interceptors: Sensory_Api_V1_Audio_AudioSynthesisClientInterceptorFactoryProtocol? = nil
  ) {
    self.fakeChannel = fakeChannel
    self.defaultCallOptions = callOptions
    self.interceptors = interceptors
  }

  /// Make a streaming response for the SynthesizeSpeech RPC. This must be called
  /// before calling 'synthesizeSpeech'. See also 'FakeStreamingResponse'.
  ///
  /// - Parameter requestHandler: a handler for request parts sent by the RPC.
  public func makeSynthesizeSpeechResponseStream(
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_SynthesizeSpeechRequest>) -> () = { _ in }
  ) -> FakeStreamingResponse<Sensory_Api_V1_Audio_SynthesizeSpeechRequest, Sensory_Api_V1_Audio_SynthesizeSpeechResponse> {
    return self.fakeChannel.makeFakeStreamingResponse(path: Sensory_Api_V1_Audio_AudioSynthesisClientMetadata.Methods.synthesizeSpeech.path, requestHandler: requestHandler)
  }

  public func enqueueSynthesizeSpeechResponses(
    _ responses: [Sensory_Api_V1_Audio_SynthesizeSpeechResponse],
    _ requestHandler: @escaping (FakeRequestPart<Sensory_Api_V1_Audio_SynthesizeSpeechRequest>) -> () = { _ in }
  ) {
    let stream = self.makeSynthesizeSpeechResponseStream(requestHandler)
    // These are the only operation on the stream; try! is fine.
    responses.forEach { try! stream.sendMessage($0) }
    try! stream.sendEnd()
  }

  /// Returns true if there are response streams enqueued for 'SynthesizeSpeech'
  public var hasSynthesizeSpeechResponsesRemaining: Bool {
    return self.fakeChannel.hasFakeResponseEnqueued(forPath: Sensory_Api_V1_Audio_AudioSynthesisClientMetadata.Methods.synthesizeSpeech.path)
  }
}

/// Handles the retrieval and management of audio models
///
/// To build a server, implement a class that conforms to this protocol.
public protocol Sensory_Api_V1_Audio_AudioModelsProvider: CallHandlerProvider {
  var interceptors: Sensory_Api_V1_Audio_AudioModelsServerInterceptorFactoryProtocol? { get }

  /// Get available models for enrollment and authentication
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  func getModels(request: Sensory_Api_V1_Audio_GetModelsRequest, context: StatusOnlyCallContext) -> EventLoopFuture<Sensory_Api_V1_Audio_GetModelsResponse>
}

extension Sensory_Api_V1_Audio_AudioModelsProvider {
  public var serviceName: Substring {
    return Sensory_Api_V1_Audio_AudioModelsServerMetadata.serviceDescriptor.fullName[...]
  }

  /// Determines, calls and returns the appropriate request handler, depending on the request's method.
  /// Returns nil for methods not handled by this service.
  public func handle(
    method name: Substring,
    context: CallHandlerContext
  ) -> GRPCServerHandlerProtocol? {
    switch name {
    case "GetModels":
      return UnaryServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_GetModelsRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_GetModelsResponse>(),
        interceptors: self.interceptors?.makeGetModelsInterceptors() ?? [],
        userFunction: self.getModels(request:context:)
      )

    default:
      return nil
    }
  }
}

#if compiler(>=5.6)

/// Handles the retrieval and management of audio models
///
/// To implement a server, implement an object which conforms to this protocol.
@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public protocol Sensory_Api_V1_Audio_AudioModelsAsyncProvider: CallHandlerProvider {
  static var serviceDescriptor: GRPCServiceDescriptor { get }
  var interceptors: Sensory_Api_V1_Audio_AudioModelsServerInterceptorFactoryProtocol? { get }

  /// Get available models for enrollment and authentication
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  @Sendable func getModels(
    request: Sensory_Api_V1_Audio_GetModelsRequest,
    context: GRPCAsyncServerCallContext
  ) async throws -> Sensory_Api_V1_Audio_GetModelsResponse
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioModelsAsyncProvider {
  public static var serviceDescriptor: GRPCServiceDescriptor {
    return Sensory_Api_V1_Audio_AudioModelsServerMetadata.serviceDescriptor
  }

  public var serviceName: Substring {
    return Sensory_Api_V1_Audio_AudioModelsServerMetadata.serviceDescriptor.fullName[...]
  }

  public var interceptors: Sensory_Api_V1_Audio_AudioModelsServerInterceptorFactoryProtocol? {
    return nil
  }

  public func handle(
    method name: Substring,
    context: CallHandlerContext
  ) -> GRPCServerHandlerProtocol? {
    switch name {
    case "GetModels":
      return GRPCAsyncServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_GetModelsRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_GetModelsResponse>(),
        interceptors: self.interceptors?.makeGetModelsInterceptors() ?? [],
        wrapping: self.getModels(request:context:)
      )

    default:
      return nil
    }
  }
}

#endif // compiler(>=5.6)

public protocol Sensory_Api_V1_Audio_AudioModelsServerInterceptorFactoryProtocol {

  /// - Returns: Interceptors to use when handling 'getModels'.
  ///   Defaults to calling `self.makeInterceptors()`.
  func makeGetModelsInterceptors() -> [ServerInterceptor<Sensory_Api_V1_Audio_GetModelsRequest, Sensory_Api_V1_Audio_GetModelsResponse>]
}

public enum Sensory_Api_V1_Audio_AudioModelsServerMetadata {
  public static let serviceDescriptor = GRPCServiceDescriptor(
    name: "AudioModels",
    fullName: "sensory.api.v1.audio.AudioModels",
    methods: [
      Sensory_Api_V1_Audio_AudioModelsServerMetadata.Methods.getModels,
    ]
  )

  public enum Methods {
    public static let getModels = GRPCMethodDescriptor(
      name: "GetModels",
      path: "/sensory.api.v1.audio.AudioModels/GetModels",
      type: GRPCCallType.unary
    )
  }
}
/// Handles all audio-related biometrics
///
/// To build a server, implement a class that conforms to this protocol.
public protocol Sensory_Api_V1_Audio_AudioBiometricsProvider: CallHandlerProvider {
  var interceptors: Sensory_Api_V1_Audio_AudioBiometricsServerInterceptorFactoryProtocol? { get }

  /// Enrolls a user with a stream of audio. Streams a CreateEnrollmentResponse as the audio is processed.
  /// CreateEnrollment only supports biometric-enabled models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  func createEnrollment(context: StreamingResponseCallContext<Sensory_Api_V1_Audio_CreateEnrollmentResponse>) -> EventLoopFuture<(StreamEvent<Sensory_Api_V1_Audio_CreateEnrollmentRequest>) -> Void>

  /// Authenticates a user with a stream of audio against an existing enrollment.
  /// Streams an AuthenticateResponse as the audio is processed.
  /// Authenticate only supports biometric-enabled models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  func authenticate(context: StreamingResponseCallContext<Sensory_Api_V1_Audio_AuthenticateResponse>) -> EventLoopFuture<(StreamEvent<Sensory_Api_V1_Audio_AuthenticateRequest>) -> Void>
}

extension Sensory_Api_V1_Audio_AudioBiometricsProvider {
  public var serviceName: Substring {
    return Sensory_Api_V1_Audio_AudioBiometricsServerMetadata.serviceDescriptor.fullName[...]
  }

  /// Determines, calls and returns the appropriate request handler, depending on the request's method.
  /// Returns nil for methods not handled by this service.
  public func handle(
    method name: Substring,
    context: CallHandlerContext
  ) -> GRPCServerHandlerProtocol? {
    switch name {
    case "CreateEnrollment":
      return BidirectionalStreamingServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_CreateEnrollmentRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_CreateEnrollmentResponse>(),
        interceptors: self.interceptors?.makeCreateEnrollmentInterceptors() ?? [],
        observerFactory: self.createEnrollment(context:)
      )

    case "Authenticate":
      return BidirectionalStreamingServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_AuthenticateRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_AuthenticateResponse>(),
        interceptors: self.interceptors?.makeAuthenticateInterceptors() ?? [],
        observerFactory: self.authenticate(context:)
      )

    default:
      return nil
    }
  }
}

#if compiler(>=5.6)

/// Handles all audio-related biometrics
///
/// To implement a server, implement an object which conforms to this protocol.
@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public protocol Sensory_Api_V1_Audio_AudioBiometricsAsyncProvider: CallHandlerProvider {
  static var serviceDescriptor: GRPCServiceDescriptor { get }
  var interceptors: Sensory_Api_V1_Audio_AudioBiometricsServerInterceptorFactoryProtocol? { get }

  /// Enrolls a user with a stream of audio. Streams a CreateEnrollmentResponse as the audio is processed.
  /// CreateEnrollment only supports biometric-enabled models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  @Sendable func createEnrollment(
    requestStream: GRPCAsyncRequestStream<Sensory_Api_V1_Audio_CreateEnrollmentRequest>,
    responseStream: GRPCAsyncResponseStreamWriter<Sensory_Api_V1_Audio_CreateEnrollmentResponse>,
    context: GRPCAsyncServerCallContext
  ) async throws

  /// Authenticates a user with a stream of audio against an existing enrollment.
  /// Streams an AuthenticateResponse as the audio is processed.
  /// Authenticate only supports biometric-enabled models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  @Sendable func authenticate(
    requestStream: GRPCAsyncRequestStream<Sensory_Api_V1_Audio_AuthenticateRequest>,
    responseStream: GRPCAsyncResponseStreamWriter<Sensory_Api_V1_Audio_AuthenticateResponse>,
    context: GRPCAsyncServerCallContext
  ) async throws
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioBiometricsAsyncProvider {
  public static var serviceDescriptor: GRPCServiceDescriptor {
    return Sensory_Api_V1_Audio_AudioBiometricsServerMetadata.serviceDescriptor
  }

  public var serviceName: Substring {
    return Sensory_Api_V1_Audio_AudioBiometricsServerMetadata.serviceDescriptor.fullName[...]
  }

  public var interceptors: Sensory_Api_V1_Audio_AudioBiometricsServerInterceptorFactoryProtocol? {
    return nil
  }

  public func handle(
    method name: Substring,
    context: CallHandlerContext
  ) -> GRPCServerHandlerProtocol? {
    switch name {
    case "CreateEnrollment":
      return GRPCAsyncServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_CreateEnrollmentRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_CreateEnrollmentResponse>(),
        interceptors: self.interceptors?.makeCreateEnrollmentInterceptors() ?? [],
        wrapping: self.createEnrollment(requestStream:responseStream:context:)
      )

    case "Authenticate":
      return GRPCAsyncServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_AuthenticateRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_AuthenticateResponse>(),
        interceptors: self.interceptors?.makeAuthenticateInterceptors() ?? [],
        wrapping: self.authenticate(requestStream:responseStream:context:)
      )

    default:
      return nil
    }
  }
}

#endif // compiler(>=5.6)

public protocol Sensory_Api_V1_Audio_AudioBiometricsServerInterceptorFactoryProtocol {

  /// - Returns: Interceptors to use when handling 'createEnrollment'.
  ///   Defaults to calling `self.makeInterceptors()`.
  func makeCreateEnrollmentInterceptors() -> [ServerInterceptor<Sensory_Api_V1_Audio_CreateEnrollmentRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse>]

  /// - Returns: Interceptors to use when handling 'authenticate'.
  ///   Defaults to calling `self.makeInterceptors()`.
  func makeAuthenticateInterceptors() -> [ServerInterceptor<Sensory_Api_V1_Audio_AuthenticateRequest, Sensory_Api_V1_Audio_AuthenticateResponse>]
}

public enum Sensory_Api_V1_Audio_AudioBiometricsServerMetadata {
  public static let serviceDescriptor = GRPCServiceDescriptor(
    name: "AudioBiometrics",
    fullName: "sensory.api.v1.audio.AudioBiometrics",
    methods: [
      Sensory_Api_V1_Audio_AudioBiometricsServerMetadata.Methods.createEnrollment,
      Sensory_Api_V1_Audio_AudioBiometricsServerMetadata.Methods.authenticate,
    ]
  )

  public enum Methods {
    public static let createEnrollment = GRPCMethodDescriptor(
      name: "CreateEnrollment",
      path: "/sensory.api.v1.audio.AudioBiometrics/CreateEnrollment",
      type: GRPCCallType.bidirectionalStreaming
    )

    public static let authenticate = GRPCMethodDescriptor(
      name: "Authenticate",
      path: "/sensory.api.v1.audio.AudioBiometrics/Authenticate",
      type: GRPCCallType.bidirectionalStreaming
    )
  }
}
/// Handles all audio event processing
///
/// To build a server, implement a class that conforms to this protocol.
public protocol Sensory_Api_V1_Audio_AudioEventsProvider: CallHandlerProvider {
  var interceptors: Sensory_Api_V1_Audio_AudioEventsServerInterceptorFactoryProtocol? { get }

  /// Validates a phrase or sound with a stream of audio.
  /// Streams a ValidateEventResponse as the audio is processed.
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  func validateEvent(context: StreamingResponseCallContext<Sensory_Api_V1_Audio_ValidateEventResponse>) -> EventLoopFuture<(StreamEvent<Sensory_Api_V1_Audio_ValidateEventRequest>) -> Void>

  /// Enrolls a sound or voice. Streams a CreateEnrollmentResponse as the audio is processed.
  /// CreateEnrollment supports all enrollable models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  func createEnrolledEvent(context: StreamingResponseCallContext<Sensory_Api_V1_Audio_CreateEnrollmentResponse>) -> EventLoopFuture<(StreamEvent<Sensory_Api_V1_Audio_CreateEnrolledEventRequest>) -> Void>

  /// Authenticates a sound or voice. Streams a ValidateEventResponse as the audio is processed.
  /// ValidateEnrolledEvent supports all enrollable models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  func validateEnrolledEvent(context: StreamingResponseCallContext<Sensory_Api_V1_Audio_ValidateEnrolledEventResponse>) -> EventLoopFuture<(StreamEvent<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest>) -> Void>
}

extension Sensory_Api_V1_Audio_AudioEventsProvider {
  public var serviceName: Substring {
    return Sensory_Api_V1_Audio_AudioEventsServerMetadata.serviceDescriptor.fullName[...]
  }

  /// Determines, calls and returns the appropriate request handler, depending on the request's method.
  /// Returns nil for methods not handled by this service.
  public func handle(
    method name: Substring,
    context: CallHandlerContext
  ) -> GRPCServerHandlerProtocol? {
    switch name {
    case "ValidateEvent":
      return BidirectionalStreamingServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_ValidateEventRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_ValidateEventResponse>(),
        interceptors: self.interceptors?.makeValidateEventInterceptors() ?? [],
        observerFactory: self.validateEvent(context:)
      )

    case "CreateEnrolledEvent":
      return BidirectionalStreamingServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_CreateEnrolledEventRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_CreateEnrollmentResponse>(),
        interceptors: self.interceptors?.makeCreateEnrolledEventInterceptors() ?? [],
        observerFactory: self.createEnrolledEvent(context:)
      )

    case "ValidateEnrolledEvent":
      return BidirectionalStreamingServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_ValidateEnrolledEventResponse>(),
        interceptors: self.interceptors?.makeValidateEnrolledEventInterceptors() ?? [],
        observerFactory: self.validateEnrolledEvent(context:)
      )

    default:
      return nil
    }
  }
}

#if compiler(>=5.6)

/// Handles all audio event processing
///
/// To implement a server, implement an object which conforms to this protocol.
@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public protocol Sensory_Api_V1_Audio_AudioEventsAsyncProvider: CallHandlerProvider {
  static var serviceDescriptor: GRPCServiceDescriptor { get }
  var interceptors: Sensory_Api_V1_Audio_AudioEventsServerInterceptorFactoryProtocol? { get }

  /// Validates a phrase or sound with a stream of audio.
  /// Streams a ValidateEventResponse as the audio is processed.
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  @Sendable func validateEvent(
    requestStream: GRPCAsyncRequestStream<Sensory_Api_V1_Audio_ValidateEventRequest>,
    responseStream: GRPCAsyncResponseStreamWriter<Sensory_Api_V1_Audio_ValidateEventResponse>,
    context: GRPCAsyncServerCallContext
  ) async throws

  /// Enrolls a sound or voice. Streams a CreateEnrollmentResponse as the audio is processed.
  /// CreateEnrollment supports all enrollable models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  @Sendable func createEnrolledEvent(
    requestStream: GRPCAsyncRequestStream<Sensory_Api_V1_Audio_CreateEnrolledEventRequest>,
    responseStream: GRPCAsyncResponseStreamWriter<Sensory_Api_V1_Audio_CreateEnrollmentResponse>,
    context: GRPCAsyncServerCallContext
  ) async throws

  /// Authenticates a sound or voice. Streams a ValidateEventResponse as the audio is processed.
  /// ValidateEnrolledEvent supports all enrollable models
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  @Sendable func validateEnrolledEvent(
    requestStream: GRPCAsyncRequestStream<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest>,
    responseStream: GRPCAsyncResponseStreamWriter<Sensory_Api_V1_Audio_ValidateEnrolledEventResponse>,
    context: GRPCAsyncServerCallContext
  ) async throws
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioEventsAsyncProvider {
  public static var serviceDescriptor: GRPCServiceDescriptor {
    return Sensory_Api_V1_Audio_AudioEventsServerMetadata.serviceDescriptor
  }

  public var serviceName: Substring {
    return Sensory_Api_V1_Audio_AudioEventsServerMetadata.serviceDescriptor.fullName[...]
  }

  public var interceptors: Sensory_Api_V1_Audio_AudioEventsServerInterceptorFactoryProtocol? {
    return nil
  }

  public func handle(
    method name: Substring,
    context: CallHandlerContext
  ) -> GRPCServerHandlerProtocol? {
    switch name {
    case "ValidateEvent":
      return GRPCAsyncServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_ValidateEventRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_ValidateEventResponse>(),
        interceptors: self.interceptors?.makeValidateEventInterceptors() ?? [],
        wrapping: self.validateEvent(requestStream:responseStream:context:)
      )

    case "CreateEnrolledEvent":
      return GRPCAsyncServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_CreateEnrolledEventRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_CreateEnrollmentResponse>(),
        interceptors: self.interceptors?.makeCreateEnrolledEventInterceptors() ?? [],
        wrapping: self.createEnrolledEvent(requestStream:responseStream:context:)
      )

    case "ValidateEnrolledEvent":
      return GRPCAsyncServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_ValidateEnrolledEventResponse>(),
        interceptors: self.interceptors?.makeValidateEnrolledEventInterceptors() ?? [],
        wrapping: self.validateEnrolledEvent(requestStream:responseStream:context:)
      )

    default:
      return nil
    }
  }
}

#endif // compiler(>=5.6)

public protocol Sensory_Api_V1_Audio_AudioEventsServerInterceptorFactoryProtocol {

  /// - Returns: Interceptors to use when handling 'validateEvent'.
  ///   Defaults to calling `self.makeInterceptors()`.
  func makeValidateEventInterceptors() -> [ServerInterceptor<Sensory_Api_V1_Audio_ValidateEventRequest, Sensory_Api_V1_Audio_ValidateEventResponse>]

  /// - Returns: Interceptors to use when handling 'createEnrolledEvent'.
  ///   Defaults to calling `self.makeInterceptors()`.
  func makeCreateEnrolledEventInterceptors() -> [ServerInterceptor<Sensory_Api_V1_Audio_CreateEnrolledEventRequest, Sensory_Api_V1_Audio_CreateEnrollmentResponse>]

  /// - Returns: Interceptors to use when handling 'validateEnrolledEvent'.
  ///   Defaults to calling `self.makeInterceptors()`.
  func makeValidateEnrolledEventInterceptors() -> [ServerInterceptor<Sensory_Api_V1_Audio_ValidateEnrolledEventRequest, Sensory_Api_V1_Audio_ValidateEnrolledEventResponse>]
}

public enum Sensory_Api_V1_Audio_AudioEventsServerMetadata {
  public static let serviceDescriptor = GRPCServiceDescriptor(
    name: "AudioEvents",
    fullName: "sensory.api.v1.audio.AudioEvents",
    methods: [
      Sensory_Api_V1_Audio_AudioEventsServerMetadata.Methods.validateEvent,
      Sensory_Api_V1_Audio_AudioEventsServerMetadata.Methods.createEnrolledEvent,
      Sensory_Api_V1_Audio_AudioEventsServerMetadata.Methods.validateEnrolledEvent,
    ]
  )

  public enum Methods {
    public static let validateEvent = GRPCMethodDescriptor(
      name: "ValidateEvent",
      path: "/sensory.api.v1.audio.AudioEvents/ValidateEvent",
      type: GRPCCallType.bidirectionalStreaming
    )

    public static let createEnrolledEvent = GRPCMethodDescriptor(
      name: "CreateEnrolledEvent",
      path: "/sensory.api.v1.audio.AudioEvents/CreateEnrolledEvent",
      type: GRPCCallType.bidirectionalStreaming
    )

    public static let validateEnrolledEvent = GRPCMethodDescriptor(
      name: "ValidateEnrolledEvent",
      path: "/sensory.api.v1.audio.AudioEvents/ValidateEnrolledEvent",
      type: GRPCCallType.bidirectionalStreaming
    )
  }
}
/// Handles all audio transcriptions
///
/// To build a server, implement a class that conforms to this protocol.
public protocol Sensory_Api_V1_Audio_AudioTranscriptionsProvider: CallHandlerProvider {
  var interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsServerInterceptorFactoryProtocol? { get }

  /// Transcribes voice into text.
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  func transcribe(context: StreamingResponseCallContext<Sensory_Api_V1_Audio_TranscribeResponse>) -> EventLoopFuture<(StreamEvent<Sensory_Api_V1_Audio_TranscribeRequest>) -> Void>
}

extension Sensory_Api_V1_Audio_AudioTranscriptionsProvider {
  public var serviceName: Substring {
    return Sensory_Api_V1_Audio_AudioTranscriptionsServerMetadata.serviceDescriptor.fullName[...]
  }

  /// Determines, calls and returns the appropriate request handler, depending on the request's method.
  /// Returns nil for methods not handled by this service.
  public func handle(
    method name: Substring,
    context: CallHandlerContext
  ) -> GRPCServerHandlerProtocol? {
    switch name {
    case "Transcribe":
      return BidirectionalStreamingServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_TranscribeRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_TranscribeResponse>(),
        interceptors: self.interceptors?.makeTranscribeInterceptors() ?? [],
        observerFactory: self.transcribe(context:)
      )

    default:
      return nil
    }
  }
}

#if compiler(>=5.6)

/// Handles all audio transcriptions
///
/// To implement a server, implement an object which conforms to this protocol.
@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public protocol Sensory_Api_V1_Audio_AudioTranscriptionsAsyncProvider: CallHandlerProvider {
  static var serviceDescriptor: GRPCServiceDescriptor { get }
  var interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsServerInterceptorFactoryProtocol? { get }

  /// Transcribes voice into text.
  /// Authorization metadata is required {"authorization": "Bearer <TOKEN>"}
  @Sendable func transcribe(
    requestStream: GRPCAsyncRequestStream<Sensory_Api_V1_Audio_TranscribeRequest>,
    responseStream: GRPCAsyncResponseStreamWriter<Sensory_Api_V1_Audio_TranscribeResponse>,
    context: GRPCAsyncServerCallContext
  ) async throws
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioTranscriptionsAsyncProvider {
  public static var serviceDescriptor: GRPCServiceDescriptor {
    return Sensory_Api_V1_Audio_AudioTranscriptionsServerMetadata.serviceDescriptor
  }

  public var serviceName: Substring {
    return Sensory_Api_V1_Audio_AudioTranscriptionsServerMetadata.serviceDescriptor.fullName[...]
  }

  public var interceptors: Sensory_Api_V1_Audio_AudioTranscriptionsServerInterceptorFactoryProtocol? {
    return nil
  }

  public func handle(
    method name: Substring,
    context: CallHandlerContext
  ) -> GRPCServerHandlerProtocol? {
    switch name {
    case "Transcribe":
      return GRPCAsyncServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_TranscribeRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_TranscribeResponse>(),
        interceptors: self.interceptors?.makeTranscribeInterceptors() ?? [],
        wrapping: self.transcribe(requestStream:responseStream:context:)
      )

    default:
      return nil
    }
  }
}

#endif // compiler(>=5.6)

public protocol Sensory_Api_V1_Audio_AudioTranscriptionsServerInterceptorFactoryProtocol {

  /// - Returns: Interceptors to use when handling 'transcribe'.
  ///   Defaults to calling `self.makeInterceptors()`.
  func makeTranscribeInterceptors() -> [ServerInterceptor<Sensory_Api_V1_Audio_TranscribeRequest, Sensory_Api_V1_Audio_TranscribeResponse>]
}

public enum Sensory_Api_V1_Audio_AudioTranscriptionsServerMetadata {
  public static let serviceDescriptor = GRPCServiceDescriptor(
    name: "AudioTranscriptions",
    fullName: "sensory.api.v1.audio.AudioTranscriptions",
    methods: [
      Sensory_Api_V1_Audio_AudioTranscriptionsServerMetadata.Methods.transcribe,
    ]
  )

  public enum Methods {
    public static let transcribe = GRPCMethodDescriptor(
      name: "Transcribe",
      path: "/sensory.api.v1.audio.AudioTranscriptions/Transcribe",
      type: GRPCCallType.bidirectionalStreaming
    )
  }
}
/// Handles synthesizing audio from text
///
/// To build a server, implement a class that conforms to this protocol.
public protocol Sensory_Api_V1_Audio_AudioSynthesisProvider: CallHandlerProvider {
  var interceptors: Sensory_Api_V1_Audio_AudioSynthesisServerInterceptorFactoryProtocol? { get }

  /// Synthesizes speech from text
  /// Authorization metadata is required {"authorization": "Bearer <TOKNE>"}
  func synthesizeSpeech(request: Sensory_Api_V1_Audio_SynthesizeSpeechRequest, context: StreamingResponseCallContext<Sensory_Api_V1_Audio_SynthesizeSpeechResponse>) -> EventLoopFuture<GRPCStatus>
}

extension Sensory_Api_V1_Audio_AudioSynthesisProvider {
  public var serviceName: Substring {
    return Sensory_Api_V1_Audio_AudioSynthesisServerMetadata.serviceDescriptor.fullName[...]
  }

  /// Determines, calls and returns the appropriate request handler, depending on the request's method.
  /// Returns nil for methods not handled by this service.
  public func handle(
    method name: Substring,
    context: CallHandlerContext
  ) -> GRPCServerHandlerProtocol? {
    switch name {
    case "SynthesizeSpeech":
      return ServerStreamingServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_SynthesizeSpeechRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_SynthesizeSpeechResponse>(),
        interceptors: self.interceptors?.makeSynthesizeSpeechInterceptors() ?? [],
        userFunction: self.synthesizeSpeech(request:context:)
      )

    default:
      return nil
    }
  }
}

#if compiler(>=5.6)

/// Handles synthesizing audio from text
///
/// To implement a server, implement an object which conforms to this protocol.
@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
public protocol Sensory_Api_V1_Audio_AudioSynthesisAsyncProvider: CallHandlerProvider {
  static var serviceDescriptor: GRPCServiceDescriptor { get }
  var interceptors: Sensory_Api_V1_Audio_AudioSynthesisServerInterceptorFactoryProtocol? { get }

  /// Synthesizes speech from text
  /// Authorization metadata is required {"authorization": "Bearer <TOKNE>"}
  @Sendable func synthesizeSpeech(
    request: Sensory_Api_V1_Audio_SynthesizeSpeechRequest,
    responseStream: GRPCAsyncResponseStreamWriter<Sensory_Api_V1_Audio_SynthesizeSpeechResponse>,
    context: GRPCAsyncServerCallContext
  ) async throws
}

@available(macOS 10.15, iOS 13, tvOS 13, watchOS 6, *)
extension Sensory_Api_V1_Audio_AudioSynthesisAsyncProvider {
  public static var serviceDescriptor: GRPCServiceDescriptor {
    return Sensory_Api_V1_Audio_AudioSynthesisServerMetadata.serviceDescriptor
  }

  public var serviceName: Substring {
    return Sensory_Api_V1_Audio_AudioSynthesisServerMetadata.serviceDescriptor.fullName[...]
  }

  public var interceptors: Sensory_Api_V1_Audio_AudioSynthesisServerInterceptorFactoryProtocol? {
    return nil
  }

  public func handle(
    method name: Substring,
    context: CallHandlerContext
  ) -> GRPCServerHandlerProtocol? {
    switch name {
    case "SynthesizeSpeech":
      return GRPCAsyncServerHandler(
        context: context,
        requestDeserializer: ProtobufDeserializer<Sensory_Api_V1_Audio_SynthesizeSpeechRequest>(),
        responseSerializer: ProtobufSerializer<Sensory_Api_V1_Audio_SynthesizeSpeechResponse>(),
        interceptors: self.interceptors?.makeSynthesizeSpeechInterceptors() ?? [],
        wrapping: self.synthesizeSpeech(request:responseStream:context:)
      )

    default:
      return nil
    }
  }
}

#endif // compiler(>=5.6)

public protocol Sensory_Api_V1_Audio_AudioSynthesisServerInterceptorFactoryProtocol {

  /// - Returns: Interceptors to use when handling 'synthesizeSpeech'.
  ///   Defaults to calling `self.makeInterceptors()`.
  func makeSynthesizeSpeechInterceptors() -> [ServerInterceptor<Sensory_Api_V1_Audio_SynthesizeSpeechRequest, Sensory_Api_V1_Audio_SynthesizeSpeechResponse>]
}

public enum Sensory_Api_V1_Audio_AudioSynthesisServerMetadata {
  public static let serviceDescriptor = GRPCServiceDescriptor(
    name: "AudioSynthesis",
    fullName: "sensory.api.v1.audio.AudioSynthesis",
    methods: [
      Sensory_Api_V1_Audio_AudioSynthesisServerMetadata.Methods.synthesizeSpeech,
    ]
  )

  public enum Methods {
    public static let synthesizeSpeech = GRPCMethodDescriptor(
      name: "SynthesizeSpeech",
      path: "/sensory.api.v1.audio.AudioSynthesis/SynthesizeSpeech",
      type: GRPCCallType.serverStreaming
    )
  }
}
